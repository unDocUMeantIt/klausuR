---
title: "Multiple Choice Test Evaluation with klausuR"
author: "m.eik michalke"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
abstract: >
  vignette: >
  %\VignetteIndexEntry{Multiple Choice Test Evaluation with klausuR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include=FALSE, cache=FALSE}
library(klausuR)
```

```{r, set-options, echo=FALSE, cache=FALSE}
options(width=85)
```

# What do you use klausuR for?

The package is intended for people or institutions that have multiple choice tests prepared and then want to report the results back to all test participants. It can deliver as results:

* Output of the overall results for all participants, including with
    * Score
    * Percent correct answers
    * Mark according to given grading scale
* Anonymous feedback (e.g. for posting the results)
* Detailed individual results for each person
    * Tabular listing of given answer, correct answer and points received for each test item
    * Both on screen and as a LaTeX report
    * Direct creation of PDF reports
    * Report can optionally include distribution graphs of results (by points and/or marks)
* Analysis of the test items, e.g.
    * Cronbach's alpha
    * Discriminatory power (part-whole-corrected)
    * Difficulty
    * Selection parameter (Lienert)
* Automatic suggestions for grading scales (e.g. according to the DIHK standard)
* Consideration of partially correct answers
* Processing tests with multiple parallel forms

Not much data is needed for these calculations either: A data set with the given answers and an answer key (correct answers for all questions). A grading scale would also be practical (which mark should be given from which number of points), but you can also have a suggestion for grading submitted, such as normal distribution of the point results or according to the DIHK standard (based on the percentage of items solved). Questions with several correct answer alternatives are possible. To get this information in the expected format, `klausuR` contains some small help functions. so ask e.g. `klausur.gen.corr()` the correct answer for each test question one after the other in order to build the answer vector from it. Optionally, questions can be weighted differently (default: one point for each question).

Another feature is help with error correction: If the test data is not already available in digital form (which is rarely the case with exams), the answers must first be typed. typos here are of course fatal. We therefore recommend typing twice independently of each other and then comparing both data sets with the function `klausur.compare()`. If it finds differences, it throws out a table with the names of the exams in question and lists the questions for which the answers differ. You can then check the exam again and eliminate the differences.

Theoretically, of course, both of them could have made the same typo by accident, so that they would not be noticed here. The individual reports are intended for this purpose (here is an [example PDF file](https://reaktanz.de/R/pckg/klausuR/ergebnisse-klausuR.pdf)), since they list in full which (typed) answers were used for the evaluation. You can attach this report to the exam and, in case of doubt, go through it question by question years later, whether something was entered incorrectly or whether there was an error in the point calculation elsewhere -- the reports make the entire evaluation completely transparent. At the same time you save the manual correction of each exam, which is certainly also associated with a risk of error.


# The workflow

The work for `klausuR` begins when your students have finished their exams and you have already transferred all answers given into digital form, i.e., a table with one row per test and one column per item and some additional meta data. This table can include as many columns as you wish, but in addition to the actual test items, it must provide at least the following columns for a test evaluation with this package:

* A unique number for each test
* The participant's family name
* The participant's first name
* A unique pseudonym or codeword (for anonymous feedback)
* Matriculation number

You should keep this in mind when preparing your table template. `klausuR` expects a data frame with the test data, so it's not important whether you store your data in a CSV, excel or SPSS file, as long as it can be imported in R. Also, the names of the columns aren't predetermined by `klausuR`, you can tell it which column is what.

## Preparing data for evaluation

Evaluating a test is done in the following steps:

#. Prepare a grading scale (marks)
#. Combine all your test meta data into one list using `klausur_meta()`
  * This includes a title and date for the test, the name of the lecturer (probably you), the correct answers etc.
#. Create formatted data object from the test data
  * Import the test data (anwsers) into a data frame; this can be omitted if your data file can be imported for evaluation directly, e.g. no further modifications are necessary
  * If you typed in all answers twice, you can use `compare()` find differences between two data frames and check back with the actual test
  * Call `klausur.data()` with the final data frame and meta data to bring everything into shape
#. Call `klausur()` on the created data object to calculate test results
#. Call `klausur.report()` on the calculated results
  * One call each for individual reports, anonymous feedback (public notice) and global results in a table
#. Optionally export the data frame with calculation results using `grand.table()`

## Preparing a grading scale

The grading scale describes which sum of points results in which mark. Currently `klausuR` does only recognize integer values as points, so you can't define half points or similar.

The format `klausuR` expects is a simple character vector with as many items as there are points to achieve. Each item is simply a string of the resulting mark. As an example, if 37 points would grand a 2.0 in your text, the 37th element of the vector would need to be `"2.0"`. Both `klausur_meta()` and `klausur.data()` also accept a simpler format and convert it automatically, see section [Manual definition] for an example.

### Manual definition

If you have already decided on a grading scale or want to re-use one from an earlier test, you can define the vector manually.

Create a named integer vector, with the item names representing the mark names and the numeric values the maximum points associated with that mark, starting with the worst mark:

```r
my_marks <- c(
  "5.0"=20,
  "4.0"=22,
  "3.7"=24,
  "3.3"=26,
  "3.0"=28,
  "2.7"=31,
  "2.3"=33,
  "2.0"=37,
  "1.7"=39,
  "1.3"=42,
  "1.0"=45
)
```

In this example, if a test subject achieved 29 points in total, that would result in mark 2.7.


### Calculate suggestions

The function `klausur.gen.marks()` can also suggest grading scales. It supports multiple formats and alorithms to calculate scales (see the manual page for more details).

Our example above uses eleven marks from 5.0 to 1.0, you would need at least 21 points to pass the test (as up to 20 points you would get a 5.0 and fail), at least 43 points for the best mark, and there's a maximum of 45 points you could achieve. Let's call `klausur.gen.marks()` to get a grading scale with roughly equal distances for this:

```r
my_marks <- klausur.gen.marks(
  mark.labels=11,
  answ=45,
  suggest=list(
    pass=21,
    best=43
  )
)
```

## Combine test meta data

This step simply is more convenient, you can actually skip it and fill in all needed information to each function manually. But you will see that some information is used repeatedly, so it is a good idea to put everything into one object and use that.

The most straight forward way to achieve this is the function `klausur_meta()`. For the most part, it just returns your input in a list, but it also generates some new variables with combined or formatted input.

`klausur_meta()` can also create the output directory if it is missing, and if `data_dir` and `data_file` are defined, it checks whether the given file can be found.


## Evaluate test results

## Generate reports

### Individual reports

### Anonymous feedback reports

## Test item analysis

## Alternative test item formats
